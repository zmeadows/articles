\documentclass[11pt]{article}
\renewcommand{\baselinestretch}{1.05}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics}
\usepackage{braket}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\renewcommand{\vec}[1]{\mathbf{#1}} % make vectors bold

\setlength\parindent{0pt}

\usepackage{hyperref}
\hypersetup{
    pdftex,
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=red
}

\usepackage{xcolor}
\newcommand{\highlight}[1]{%
  \colorbox{yellow!50}{$\displaystyle#1$}}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\begin{document}

\title{Green's Functions}
\author{Zachary A. Meadows \\ \textit{University of Massachusetts Amherst}}
\maketitle
\tableofcontents

\section{Introduction}

Green's functions induce more confusion in the first few semesters of physics grad school than perhaps any other mathematical topic. The wide variety of ways in which they can be derived, applied, and conceptuatlized is an outcome of their usefulness, but simultaneously a source of frustration when encountering them for the first time. In this article I hope to convey as succinctly as possible, with only as much mathematical flourish as necessary, a core understanding of Green's functions which may save a new physics grad student precious time and energy. Let us begin.

\subsection{Motivation}
Before we develop the mathematical techniques for actually \textit{solving} for Green's functions, lets take a step back and consider why they are so useful. Suppose we have a physical system whose behavior can be modelled by some known differential equation. As is typical in physics, these equations are often written with the LHS (Left Hand Side) containing terms \textit{built in} to the system itself and the RHS (Right Hand Side) representing some \textit{external} force/source/sink/etc. Consider a few examples you have undoubtedly seen before, and which we will cover in detail later:
%
\begin{align}
    m\ddot x(t) + \gamma \dot x(t) + kx(t) &= F_{ext}(t) \label{eq:SHO} \\
    \nabla^2 \phi(\vec r, t) - \frac{1}{c^2}\frac{\partial^2 \phi(\vec r, t)}{\partial t^2} &= -4\pi f(\vec r, t) \label{eq:WAVE} \\
    \frac{\partial u}{\partial t} - \alpha \nabla^2 u &= Q(\vec r, t) \label{eq:HEAT}
\end{align}

In these familiar equations, one can intuitively think of the the RHS terms as some sort of external perturbation and the LHS as representative of the manner in which the system \textit{encodes} the effect of this perturbation within the trajectory/wave/heat profile/etc. In the case of the damped and driven harmonic oscillator \eqref{eq:SHO}, $F_{ext}(t)$ might be the force generated by some machine attached by a rod to a spring/mass system. In the wave equation \eqref{eq:WAVE}, $f(\vec r,t)$ would represent the electric charge distribution of electrodynamics. In the heat equation \eqref{eq:HEAT}, $Q(\vec r, t)$ represents heat flux in/out of the system (again via some sort of machine or other means).

Now, if we suppose the LHS of these equations encode the unchangeable, fundamental character of how a particular system responds to external perturbations, we might assume there could be a more direct way of relating the final solution we are after $(x(t), \phi(\vec r,t),u(\vec r,t))$ to the external perturbation rather than going through the machinery of solving the differential equation for each unique form of the perturbation term that we encounter. This is where the Green's function comes in. A Green's function (G) is an \textbf{integral kernel} which can be used to solve an \textbf{inhomogeneous differential equation} with \textbf{specified boundary conditions}. In other words, the solution to our three examples may be written:

\begin{align}
    x(t) &= \int_0^{\infty} G(t,t') F_{ext}(t') dt' \\
    \phi(\vec r,t) &= \int_V \int_0^{\infty} G(\vec r, t; \vec r', t') f(\vec r', t') d\vec r' dt' \\
    u(\vec r,t) &= \int_V \int_0^{\infty} G(\vec r, t; \vec r', t') Q(\vec r', t') d\vec r' dt'
\end{align}


\section{A Discrete Analogue}

Suppose for the sake of a computer simulation the trajectory of a particle in 1 dimension is defined as a sequence of $N$ real numbers separated by a time step $\Delta h$. Thus the total time of the simulation is $\left( N-1 \right)\Delta h$ and the trajectory can be represented as a vector:

\begin{align}
\vec x = \colvec{5}{x(t=0)}{x(t=\Delta h)}{x(t=2\Delta h)}{\vdots}{x(t=\left( N-1 \right) \Delta h)} \equiv \colvec{5}{x_1}{x_2}{x_3}{\vdots}{x_N} \in \mathbb{R}^N
\end{align}

For this particular simulation (the particulars of the system being simulated are not important) the time evolution of the trajectory is governed by a matrix equation:

\begin{align}
    \hat L \vec x = \vec f
\end{align}

where $\hat L$ is a nonsingular (invertible) matrix  and $\vec f$ is an $N \times 1$ vector of constant elements. Though we are working within a discrete domain rather than a continuous one, this situation closely resembles what is found in equations \eqref{eq:SHO}, \eqref{eq:WAVE}, and \eqref{eq:HEAT}. In terms of the indexed elements of the matrices and vectors:

\begin{align}
    \sum\limits_{i=1}^N \hat L_{ij} x_j = f_i
\end{align}

So far this is a simple matrix equation, but by analogy with the earlier discussion we can begin to think of $\vec f$ as an external perturbation of the trajectory and $\hat L$ as description of how the system \textit{encodes} this external perturbation onto the trajectory of the particle. Because $\hat L$ is invertible, we can write:

\begin{align}
    x_i = \sum\limits_{i=1}^N \hat L_{ij}^{-1} f_j
\end{align}

All the elements of $\hat L$ and $\vec f$ are known. It follows that the trajectory $\vec x$ is uniquely and completely determined by $\hat L, \vec f$ and $x_1$ (an initial condition). Based on physical consideration we can deduce certain properties of $\hat L^{-1}$ which will prove to be useful.

\subsection{Causality}

The matrix element $L_{ij}^{-1}$ represents the degree to which the trajectory $\vec x$ at time $i$ ($x_i$)  responds to a unit impulse at time $j$.

\subsection{Translational Invariance}

\section{Background}
\subsection{Generalized Functions}
\subsection{Dirac Delta Function}
\subsection{Heaviside Step Function}


\section{Differential Equations}
\subsection{Time-Independent}

Consider the differential equation

\begin{align}
    \hat L u(\vec x) = f(\vec x) \label{eq:SLE}
\end{align}

where $\hat L$ is a Sturm Louiville Operator

\begin{align}
    \hat L \left[ u\left( \vec x \right) \right] = \frac{1}{w(\vec x)} \nabla \cdot \left[ w(\vec x) p(\vec x) \nabla u(\vec x) \right] + q(\vec x)u(\vec x)
\end{align}

and $u(\vec x): \mathbb{R}^n \rightarrow \mathbb{R}$ obeys some combination of canonical boundary conditions on its domain: $\vec x \in \chi \subseteq \mathbb{R}^n$. Many differential equations in physics fall into this broad category. By the Spectral Theorem of functional analysis, the solution can be written as a linear combination of eigenfunctions of $\hat L$ which obey the specified boundary conditions.

\begin{align}
    \hat L e_m(\vec x) = \lambda_m e_m(\vec x) \\
    u(\vec x) = \sum\limits_m u_m e_m(\vec x)
\end{align}

If we then take the inner product of both sides of \eqref{eq:SLE} with respect to a particular eigenfunction $e_k(\vec x)$, a useful result emerges:

\begin{align}
    \Braket{e_k(\vec x) | \hat L u(\vec x)} &= \Braket{e_k(\vec x) | f(\vec x)} \\
    \Braket{e_k(\vec x) | \sum\limits_m \lambda_m u_m e_m(\vec x)} &= \Braket{e_k(\vec x) | f(\vec x)} \\
    \lambda_m u_m &= \Braket{e_k(\vec x) | f(\vec x)}
\end{align}

It follows that we can write the solution as

\begin{align}
    u(\vec x) &= \sum\limits_m \frac{\Braket{e_m(\vec x) | f(\vec x)}}{\lambda_m} e_m(\vec x)
\end{align}

The inner product in this case is defined as

\begin{align}
    \Braket{f(\vec x)|g(\vec x)} = \int_{\chi} w(\vec x_0) f^*(\vec x_0) g(\vec x_0) d^n\vec x_0
\end{align}

which leads to

\begin{align}
    u(\vec x) &= \sum\limits_m \frac{1}{\lambda_m} \left[ \int_{\chi} w(\vec x_0) e_m^*(\vec x_0) f(\vec x_0) d^n\vec x_0 \right] e_m(\vec x)
\end{align}

Now we transpose the order of integration and summation, a mathematically brazen technique that turns out to work (but with some consequences).

\begin{align}
u(\vec x) &= \int_{\chi} \left[ \sum\limits_m \frac{1}{\lambda_m} w(\vec x_0) e_m^*(\vec x_0) e_m(\vec x) \right] f(\vec x_0) d^n\vec x_0
\end{align}

This Green's function is defined as this sum:

\begin{align}
    \highlight{G(\vec x,\vec x_0) \equiv \sum\limits_m \frac{1}{\lambda_m} w(\vec x_0) e_m^*(\vec x_0) e_m(\vec x)}
\end{align}

This is not the method by which Green's functions are typically calculated, but it does provide a strong foundation assuming they exist for the differential equations we use in physics.

\subsection{Time-Dependent}

\section{Examples}
\subsection{Electrostatics}
\subsection{Driven Oscillator}
\begin{align}
    m \ddot x(t) + kx(t) &= F_{ext}(t)
\end{align}


\subsection{Damped, Driven  Oscillator}

Consider the system
\begin{align}
    m \ddot x(t) + \gamma \dot x(t) + kx(t) &= F_{ext}(t) \label{eq:DDHO}
\end{align}
subject to the boundary conditions

\begin{align}
    x(t < 0) &= 0 \\
    x(t=0) &= x_0 \label{eq:X0}\\
    \dot x(t = 0) &= v_0 \label{eq:V0}
\end{align}

Our goal then is to solve the following inhomogenous ordinary differential equation:
\begin{align}
    m \ddot G(t) + \gamma \dot G(t) + kG(t) &= \delta(t) \label{eq:DDHOG}
\end{align}


In this case it turns out to be easier to work with the Fourier representation:

\begin{align}
    G(t) &= \frac{1}{2\pi} \int_{-\infty}^{\infty} g(\omega) e^{i\omega t} d\omega \label{eq:DDHOGF} \\
    \delta(t) &= \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{i\omega t} d\omega \label{eq:DF}
\end{align}

By plugging \eqref{eq:DDHOGF} and \eqref{eq:DF} into \eqref{eq:DDHOG}, applying deriatives are rearranging terms:

\begin{align}
    \int_{-\infty}^{\infty} \left[ \left\{ -m\omega^2 + i\omega \gamma + k \right\}g(\omega) - 1 \right] e^{i\omega t} d\omega = 0
\end{align}

Since the basis functions $e^{i\omega t}$ of the fourier representation form an orthonormal basis, it follows that

\begin{align}
    \forall \omega, \ \ \ \left\{ -m\omega^2 + i\omega \gamma + k \right\}g(\omega) - 1 = 0 \\
    \Rightarrow G(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \frac{e^{i\omega t}}{-m\omega^2 + i\omega \gamma + k} dw
\end{align}

In preparation for solving the integral via residue calculus, we factor the denominator

\begin{align}
    G(t) &= \frac{1}{2\pi}\oint_C \frac{e^{i\omega t}}{\left( \omega-\Omega_+ \right) \left( \omega - \Omega_- \right)} d\omega
\end{align}

where $\Omega_{\pm} = i\frac{\gamma}{2m} \mp \sqrt{\frac{k}{m}-\frac{\gamma^2}{4m^2}}$ and $C$ is the UHP infinite semi-circle.  Both roots are in the upper half plane no matter the relation between $k$ and $\gamma$, so the residues of both poles must be evaluated:

\begin{align}
    G(t) &= i \left[ \frac{e^{i\Omega_+ t} - e^{i \Omega_- t}}{\Omega_- - \Omega_+} \right] \\
    &= \frac{e^{\frac{-\gamma}{2m} t}}{\sqrt{\frac{k}{m}-\frac{\gamma^2}{4m^2}}} \frac{1}{2i} \left( e^{i \sqrt{\frac{k}{m}-\frac{\gamma^2}{4m^2}} t} - e^{-i \sqrt{\frac{k}{m}-\frac{\gamma^2}{4m^2}} t} \right) \\
    &\equiv \frac{e^{-bt}}{\omega_1}\sin{\omega_1 t}
\end{align}

where $b \equiv \frac{\gamma}{2m}$ and $\omega_1 \equiv \sqrt{\frac{k}{m}-\frac{\gamma^2}{4m^2}}$. By taking $t \rightarrow t-t_0$ and imposiing  the causality condition $G(t<0)=0$ we have our final solution:


\begin{align}
    \highlight{G(t-t_0) = \theta(t-t_0) \frac{e^{-b\left( t-t_0 \right)}}{\omega_1}\sin\left[ \omega_1 \left( t-t_0 \right) \right]}
\end{align}

In order to account for boundary conditions \eqref{eq:X0} and \eqref{eq:V0}, we must write the full solution $x(t)$ as a combination of complimentary (transient) and particular (steady state) solutions $x(t) = x_p(t) + x_c(t)$

\begin{align}
    x_p(t) &= \int_{0}^{t} \frac{e^{-b\left( t-t_0 \right)}}{\omega_1}\sin\left[ \omega_1 \left( t-t_0 \right) \right] f(t_0) dt_0 \\
    x_c(t) &=\theta(t) \  e^{-bt}\left[ \frac{x_0\left( \omega_1+b \right)+v_0  }{2\omega_1}e^{\omega_1 t} + \frac{x_0\left( \omega_1-b \right)-v_0  }{2\omega_1}e^{-\omega_1 t} \right]
\end{align}

where the the action of $\theta(t-t_0)$ has transformed the limits of integration. $x_c(t)$ is merely the familiar (though perhaps no written in precisely this way) solution to the homogeneous equivalent of \eqref{eq:DDHO}, with $\theta(t)$ included to account for our continued assumption that $x(t<0) = 0$.



\subsection{Helmholtz Equation}

\begin{align}
\left( \nabla^2+k^2 \right)\phi(\vec r) = -4\pi f(\vec r) \\
\left( \nabla^2+k^2 \right)G_k(\vec r) = -4\pi \delta(\vec r)
\end{align}

Here the subscript $k$ in $G_k(\vec r)$ indicates the dependence  of the Green's function on the particular value of k. This will aid us in deriving the Green's function of the wave equation later. If we assume there are no boundary surfaces, the Green's function can only depend on $r \equiv |\vec r - \vec r'|$ and we can exploit the simplicity of the spherically symmetric Laplacian:

\begin{align}
    \frac{1}{r} \frac{d^2}{dr^2} \left( rG(r) \right) + k^2 G(r) = -4\pi \delta(r)
\end{align}

after multiplying by r,

\begin{align}
    \frac{d^2}{dr^2} \left( rG(r) \right) + k^2 \left( rG(r)  \right) = -4\pi r \delta(r)
\end{align}

When $r \neq 0$, $rG(r)$ satisifies $ \frac{d^2}{dr^2} \left( rG(r) \right) + k^2 \left( rG(r)  \right) = 0$, the equation for a simple harmonic oscillator with solutions $rG(r) = A e^{\pm ikr} \Rightarrow \ G(r) = A \frac{e^{\pm ikr}}{r}$, where $A$ is a constant.  All that is left to do is to ensure this solution satisfies the inhomogeneous equation and find the value of $A$ in the process.

\begin{align}
    \left( \nabla^2+k^2 \right) A \frac{e^{\pm ikr}}{r} &= -4\pi \delta(r)
\end{align}
\begin{align}
    A \left[ e^{\pm ikr} \nabla^2 \left(\frac{1}{r}\right) + \frac{1}{r} \nabla^2 \left( e^{\pm ikr}  \right) + 2 \nabla \left( \frac{1}{r} \right) \cdot \nabla \left( e^{\pm ikr} \right) \right] + A k^2 \frac{e^{\pm ikr}}{r} &= -4\pi \delta(r)
\end{align}

The second and fourth terms on the LHS cancel, and we are left with

\begin{align}
    A e^{\pm ikr} \left[ \nabla^2 \left(\frac{1}{r}\right) \pm 2ik  \hat r \cdot \nabla \left( \frac{1}{r} \right) \right] &= -4\pi \delta(r)
\end{align}

Since the delta function only acts exactly at $r=0$, we take the limit as $r \rightarrow 0$. The first term dominates in this limit, which leaves us with

\begin{align}
    A \left[ \nabla^2 \left(\frac{1}{r}\right) \right] &= -4\pi \delta(r)
\end{align}

which demands that the constant A=1 and confirms that our solution does in fact hold in the presence of the delta function. The two different solutions

\begin{align}
    \highlight{G_k^{\left( \pm \right)} = \frac{e^{\pm ikr}}{r} }
\end{align}

represent \textit{sources} $\left( G_k^+ \right)$ and \textit{sinks} $\left( G_k^- \right)$.






\subsection{Wave Equation}
\subsection{Free Particle (QM)}

\section{Distribution Theory \& Fundamental Solution}
\section{Resources}

\end{document}

